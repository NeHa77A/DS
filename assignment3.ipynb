{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7876c8f5-a67c-4288-ba31-33e91ad3b342",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expecting 1 items, got 100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal sum: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_sum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 42\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Scatter the array to all processes\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m array \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mscatter(array, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Calculate partial sum\u001b[39;00m\n\u001b[0;32m     27\u001b[0m partial_sum \u001b[38;5;241m=\u001b[39m calculate_partial_sum(array, rank, size)\n",
      "File \u001b[1;32mmpi4py\\MPI\\Comm.pyx:1587\u001b[0m, in \u001b[0;36mmpi4py.MPI.Comm.scatter\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmpi4py\\MPI\\msgpickle.pxi:822\u001b[0m, in \u001b[0;36mmpi4py.MPI.PyMPI_scatter\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmpi4py\\MPI\\msgpickle.pxi:160\u001b[0m, in \u001b[0;36mmpi4py.MPI.pickle_dumpv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expecting 1 items, got 100"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def calculate_partial_sum(array, rank, size):\n",
    "    N = len(array)\n",
    "    chunk_size = N // size\n",
    "    start = rank * chunk_size\n",
    "    end = start + chunk_size if rank != size - 1 else N\n",
    "    partial_sum = np.sum(array[start:end])\n",
    "    return partial_sum\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "\n",
    "    if rank == 0:\n",
    "        N = 100  # total number of elements\n",
    "        array = np.arange(N)\n",
    "    else:\n",
    "        array = None\n",
    "\n",
    "    # Scatter the array to all processes\n",
    "    array = comm.scatter(array, root=0)\n",
    "\n",
    "    # Calculate partial sum\n",
    "    partial_sum = calculate_partial_sum(array, rank, size)\n",
    "\n",
    "    # Gather all partial sums\n",
    "    partial_sums = comm.gather(partial_sum, root=0)\n",
    "\n",
    "    # Display intermediate sums calculated at different processors\n",
    "    if rank == 0:\n",
    "        print(\"Intermediate sums calculated at different processors:\")\n",
    "        for i, p_sum in enumerate(partial_sums):\n",
    "            print(f\"Processor {i}: {p_sum}\")\n",
    "\n",
    "        total_sum = np.sum(partial_sums)\n",
    "        print(f\"Total sum: {total_sum}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef34887-6250-4aee-bf39-531edc0b2b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mpi4py\n",
      "  Downloading mpi4py-3.1.6-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Downloading mpi4py-3.1.6-cp311-cp311-win_amd64.whl (466 kB)\n",
      "   ---------------------------------------- 0.0/466.1 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/466.1 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 153.6/466.1 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 399.4/466.1 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 466.1/466.1 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: mpi4py\n",
      "Successfully installed mpi4py-3.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mpi4py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ede9ed-9d78-4026-aff1-b0bda57b8635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate sums calculated at different processors:\n",
      "Processor 0: 4950\n",
      "Total sum: 4950\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def calculate_partial_sum(array_chunk):\n",
    "    return np.sum(array_chunk)\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "\n",
    "    if rank == 0:\n",
    "        N = 100  # total number of elements\n",
    "        array = np.arange(N)\n",
    "        chunk_size = len(array) // size\n",
    "        # Scatter the array into chunks\n",
    "        scatter_data = [array[i:i+chunk_size] for i in range(0, len(array), chunk_size)]\n",
    "    else:\n",
    "        scatter_data = None\n",
    "\n",
    "    # Scatter the array chunks to all processes\n",
    "    local_chunk = comm.scatter(scatter_data, root=0)\n",
    "\n",
    "    # Calculate partial sum\n",
    "    partial_sum = calculate_partial_sum(local_chunk)\n",
    "\n",
    "    # Gather all partial sums\n",
    "    partial_sums = comm.gather(partial_sum, root=0)\n",
    "\n",
    "    # Display intermediate sums calculated at different processors\n",
    "    if rank == 0:\n",
    "        print(\"Intermediate sums calculated at different processors:\")\n",
    "        for i, p_sum in enumerate(partial_sums):\n",
    "            print(f\"Processor {i}: {p_sum}\")\n",
    "\n",
    "        total_sum = np.sum(partial_sums)\n",
    "        print(f\"Total sum: {total_sum}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fd73d1-8aa2-46d1-a2e1-dbbd1fd73a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate sums calculated at different processors:\n",
      "Processor 0: 4950\n",
      "Total sum: 4950\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def calculate_partial_sum(array_chunk):\n",
    "    return np.sum(array_chunk)\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "\n",
    "    if rank == 0:\n",
    "        N = 100  # total number of elements\n",
    "        array = np.arange(N)\n",
    "        chunk_size = len(array) // size\n",
    "        # Scatter the array into chunks\n",
    "        scatter_data = [array[i:i+chunk_size] for i in range(0, len(array), chunk_size)]\n",
    "    else:\n",
    "        scatter_data = None\n",
    "\n",
    "    # Scatter the array chunks to all processes\n",
    "    local_chunk = comm.scatter(scatter_data, root=0)\n",
    "\n",
    "    # Calculate partial sum\n",
    "    partial_sum = calculate_partial_sum(local_chunk)\n",
    "\n",
    "    # Gather all partial sums\n",
    "    partial_sums = comm.gather((rank, partial_sum), root=0)\n",
    "\n",
    "    # Display intermediate sums calculated at different processors\n",
    "    if rank == 0:\n",
    "        print(\"Intermediate sums calculated at different processors:\")\n",
    "        for i, (p_rank, p_sum) in enumerate(partial_sums):\n",
    "            print(f\"Processor {p_rank}: {p_sum}\")\n",
    "\n",
    "        total_sum = np.sum([p_sum for _, p_sum in partial_sums])\n",
    "        print(f\"Total sum: {total_sum}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c17cd1a-5e2f-46bf-9211-e3584588ad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor 0: Partial sum calculated = 4950\n",
      "Intermediate sums calculated at different processors:\n",
      "Processor 0: 4950\n",
      "Total sum: 4950\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def calculate_partial_sum(array_chunk, rank):\n",
    "    partial_sum = np.sum(array_chunk)\n",
    "    print(f\"Processor {rank}: Partial sum calculated = {partial_sum}\")\n",
    "    return partial_sum\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "\n",
    "    if rank == 0:\n",
    "        N = 100  # total number of elements\n",
    "        array = np.arange(N)\n",
    "        chunk_size = len(array) // size\n",
    "        # Scatter the array into chunks\n",
    "        scatter_data = [array[i:i+chunk_size] for i in range(0, len(array), chunk_size)]\n",
    "    else:\n",
    "        scatter_data = None\n",
    "\n",
    "    # Scatter the array chunks to all processes\n",
    "    local_chunk = comm.scatter(scatter_data, root=0)\n",
    "\n",
    "    # Calculate partial sum\n",
    "    partial_sum = calculate_partial_sum(local_chunk, rank)\n",
    "\n",
    "    # Gather all partial sums\n",
    "    partial_sums = comm.gather(partial_sum, root=0)\n",
    "\n",
    "    # Display intermediate sums calculated at different processors\n",
    "    if rank == 0:\n",
    "        print(\"Intermediate sums calculated at different processors:\")\n",
    "        for i, p_sum in enumerate(partial_sums):\n",
    "            print(f\"Processor {i}: {p_sum}\")\n",
    "\n",
    "        total_sum = np.sum(partial_sums)\n",
    "        print(f\"Total sum: {total_sum}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d73a3-6797-43f4-9cc8-73e5155b349c",
   "metadata": {},
   "source": [
    "# main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd63f36e-4f48-4d41-8782-759b2b5bc456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array: 1 2 3 4 5 6 7 8 9 10 \n",
      "Processor 0 local sum: 55\n",
      "Global sum: 55\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    N = 10  # Total number of elements\n",
    "    array = np.zeros(N, dtype=int)  # Initialize the array\n",
    "    local_sum = 0  # Partial sum for each processor\n",
    "    global_sum = 0  # Final sum\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"Original array:\", end=\" \")\n",
    "        for i in range(N):\n",
    "            array[i] = i + 1\n",
    "            print(array[i], end=\" \")\n",
    "        print()\n",
    "\n",
    "    # Scatter the array elements to all processors\n",
    "    local_array = np.zeros(N // size, dtype=int)\n",
    "    comm.Scatter(array, local_array, root=0)\n",
    "\n",
    "    # Calculate local sum\n",
    "    for i in range(len(local_array)):\n",
    "        local_sum += local_array[i]\n",
    "\n",
    "    # Display local sums calculated by each processor\n",
    "    print(f\"Processor {rank} local sum: {local_sum}\")\n",
    "\n",
    "    # Reduce all local sums to get the global sum\n",
    "    global_sum = comm.reduce(local_sum, op=MPI.SUM, root=0)\n",
    "\n",
    "    # Display the global sum\n",
    "    if rank == 0:\n",
    "        print(\"Global sum:\", global_sum)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b7b031-ca00-45fd-8659-f6fb21754901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 5 elements:\n",
      "Intermediate sum at process 0 is 10\n",
      "Final sum: 10\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "\n",
    "    unitsize = 5\n",
    "    root = 0\n",
    "\n",
    "    send_buffer = None\n",
    "    recieve_buffer = np.zeros(unitsize, dtype=int)\n",
    "    new_recieve_buffer = np.zeros(size, dtype=int)\n",
    "\n",
    "    # Set data for distribution\n",
    "    if rank == root:\n",
    "        total_elements = unitsize * size\n",
    "        print(f\"Enter {total_elements} elements:\")\n",
    "        send_buffer = np.arange(total_elements)\n",
    "    \n",
    "    # Scatter data to processes\n",
    "    comm.Scatter(send_buffer, recieve_buffer, root=root)\n",
    "\n",
    "    # Calculate sum at non-root processes\n",
    "    # Store result in first index of array\n",
    "    recieve_buffer[0] = np.sum(recieve_buffer)\n",
    "\n",
    "    print(f\"Intermediate sum at process {rank} is {recieve_buffer[0]}\")\n",
    "\n",
    "    # Gather data from processes\n",
    "    comm.Gather(recieve_buffer[0:1], new_recieve_buffer, root=root)\n",
    "\n",
    "    # Aggregate output from all non-root processes\n",
    "    if rank == root:\n",
    "        total_sum = np.sum(new_recieve_buffer)\n",
    "        print(f\"Final sum: {total_sum}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae597d-7b6c-4ed5-b425-c7acd69ffe79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
